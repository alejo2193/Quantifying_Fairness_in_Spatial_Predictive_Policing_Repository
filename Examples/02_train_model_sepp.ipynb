{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Notebook: Predictive Model Evaluation (SEPP)\n",
    "\n",
    "## Purpose\n",
    "This notebook is designed to train and evaluate the Self-Exciting Point Process (SEPP) predictive model on spatio-temporal data splits. The primary goal is to assess its performance using the Earth Mover's Distance (EMD) metric, potentially exploring different parameters.\n",
    "\n",
    "## Workflow Stage\n",
    "This notebook is in the **Model Training and Evaluation** stage of the data science workflow. It takes processed data splits, trains the SEPP model, generates predictions, and evaluates them against real data.\n",
    "\n",
    "## About\n",
    "This notebook processes multiple data splits to evaluate the SEPP prediction model. The results are saved for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "This section handles the initial setup, including mounting Google Drive (for environments like Colab), importing necessary system libraries, and configuring the Python path to include custom library directories. This is crucial for accessing project-specific modules and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30891,
     "status": "ok",
     "timestamp": 1694723851383,
     "user": {
      "displayName": "Diego Alejandro Hernandez Castaneda",
      "userId": "18178177893889256336"
     },
     "user_tz": 300
    },
    "id": "dU6EyFvoehee",
    "outputId": "e5cae2cb-248a-471f-8714-662dfdf4cf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for environment setup\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "\n",
    "# Mount Google Drive. This is specific to Google Colab.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define base path for shared drive resources (replace with a variable if possible, or standardize)\n",
    "# Note: Hardcoding drive paths can reduce portability. Consider using relative paths or environment variables.\n",
    "# Example of standardized path base (requires defining a base directory variable, e.g., base_project_root):\n",
    "# base_project_root = '/path/to/your/project'\n",
    "# path_opencp = os.path.join(base_project_root, 'Librerias', 'PredictCode')\n",
    "# path_fairness = os.path.join(base_project_root, 'Librerias')\n",
    "# path_exp = os.path.join(base_project_root, 'Notebooks_for_topic', 'EXP0', 'SCRIPTS')\n",
    "\n",
    "# Original hardcoded paths - replace with standardized approach if possible\n",
    "path_opencp=\"drive/.../Libraries/PredictCode/\"\n",
    "path_fairness=\"drive/.../Libraries/\"\n",
    "path_exp=\"drive/.../Examples/\"\n",
    "\n",
    "# Add custom library directories to the system path\n",
    "# This allows importing modules from these directories\n",
    "sys.path.insert(0, os.path.abspath(path_opencp))\n",
    "sys.path.insert(0, os.path.abspath(path_fairness))\n",
    "sys.path.insert(0, os.path.abspath(path_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Library Imports and Initial Configuration\n",
    "\n",
    "This cell imports the core libraries and custom modules required for the analysis, including data handling, numerical operations, date/time manipulations, the `open_cp` library for spatio-temporal analysis, and specific model implementations. It also changes the current working directory to the project's base experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12124,
     "status": "ok",
     "timestamp": 1694723877750,
     "user": {
      "displayName": "Diego Alejandro Hernandez Castaneda",
      "userId": "18178177893889256336"
     },
     "user_tz": 300
    },
    "id": "16cUdPzpnbBw",
    "outputId": "a49a7b06-cfe1-461a-f419-e901db9a0b26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load 'descartes' package.\n",
      "ERROR:open_cp.geometry:Failed to import `rtree`.\n",
      "ERROR:open_cp.network:Failed to import `rtree`.\n"
     ]
    }
   ],
   "source": [
    "# Import data handling and numerical libraries\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Import the open_cp library for spatio-temporal analysis\n",
    "import open_cp\n",
    "\n",
    "# Import Colab-specific output clearing utility\n",
    "from google.colab import output\n",
    "\n",
    "# Change the current working directory to the experiment base directory\n",
    "# Note: Hardcoding paths like this can reduce portability. \n",
    "# Consider defining a base directory variable (e.g., using os.path.join with a root path)\n",
    "# Example: os.chdir(os.path.join(base_project_root, 'Notebooks_for_topic', 'EXP0'))\n",
    "os.chdir(\"drive/.../Examples/\")\n",
    "\n",
    "# Import specific predictive models from a custom module\n",
    "# These model classes are used for training and prediction\n",
    "from models.model_selection import NAIVE_MODEL, KDE_MODEL, SEPP_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXMoicv4nJhL"
   },
   "source": [
    "## 3. Variable and Region Setup\n",
    "\n",
    "This section sets up key variables required for the analysis, including the spatial region of interest and parameters imported from a `global_vars` file. This centralizes configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alULHi4WnPOr"
   },
   "outputs": [],
   "source": [
    "# Import necessary functions and variables from custom modules\n",
    "from open_cp.sources.sepp import make_time_unit\n",
    "\n",
    "# Import global variables for region definition and grid size\n",
    "# These variables are expected to be defined in global_vars.py\n",
    "from global_vars import x_min,x_max,y_min,y_max,grid_size,days_time_unit\n",
    "\n",
    "# Define the rectangular region of interest based on imported bounds\n",
    "region = open_cp.RectangularRegion(x_min,x_max, y_min,y_max)\n",
    "\n",
    "# Commented out parameters (cut_time, cut_space) - potentially for other models or experiments\n",
    "# cut_time=24*15\n",
    "# cut_space=0.5\n",
    "\n",
    "# Import more global variables for directory paths and final training date\n",
    "from global_vars import dir_sims, dir_split, f_final_train\n",
    "\n",
    "# Re-import model classes (redundant if already imported, but kept as in original)\n",
    "from models.model_selection import NAIVE_MODEL, KDE_MODEL, SEPP_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Prediction and Evaluation\n",
    "\n",
    "This cell defines essential helper functions used throughout the notebook to process model predictions, obtain real data in a comparable grid format, and calculate the Earth Mover's Distance (EMD) between predicted and real intensity matrices. These functions encapsulate key logic for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viiSJdVTuppu"
   },
   "outputs": [],
   "source": [
    "# Define a function to get the intensity matrix from a model prediction\n",
    "# Takes a model object and a prediction date as input\n",
    "def intensity_m(model, date, region=region, grid_size=grid_size):\n",
    "    # Attempt to get the prediction for the given date\n",
    "    try:\n",
    "        predict = model.predict(date)\n",
    "    except:\n",
    "        # If predicting requires no date argument, call predict() without it\n",
    "        predict = model.predict()\n",
    "\n",
    "    # Attempt to convert the prediction to a GridPredictionArray and get the intensity matrix\n",
    "    try:\n",
    "        M = open_cp.predictors.GridPredictionArray.from_continuous_prediction_region(predict, region, grid_size, grid_size)\n",
    "        intensity_matrix = M.intensity_matrix\n",
    "    except:\n",
    "        # If the prediction object already has an intensity matrix, use it\n",
    "        intensity_matrix = predict.intensity_matrix\n",
    "\n",
    "    # Check if the sum of the intensity matrix is zero\n",
    "    if intensity_matrix.sum() == 0:\n",
    "        # If sum is zero, return the matrix as is (no intensity)\n",
    "        return intensity_matrix\n",
    "    else:\n",
    "        # If sum is not zero, return the matrix normalized so the sum is 1\n",
    "        return intensity_matrix / intensity_matrix.sum()\n",
    "\n",
    "# Import necessary library for naive counting grid kernel\n",
    "import open_cp.naive as naive\n",
    "\n",
    "# Define a function to get the real event intensity matrix for a given date\n",
    "# Takes timed event points, a prediction date, grid size, and region as input\n",
    "def get_real_m(timedpoints, p_date, grid_size=grid_size, region=region):\n",
    "    # Filter events that occurred within the 24-hour period starting from p_date\n",
    "    real = timedpoints[(timedpoints.times_datetime() >= p_date) & ((timedpoints.times_datetime() < p_date + timedelta(days=1)))]\n",
    "\n",
    "    # Create a CountingGridKernel predictor with the specified grid size and region\n",
    "    predictor = naive.CountingGridKernel(grid_size, region=region)\n",
    "\n",
    "    # Set the filtered real events as the data for the predictor\n",
    "    predictor.data = real\n",
    "\n",
    "    # Attempt to get the grid prediction and renormalize it\n",
    "    try:\n",
    "        gridpred = predictor.predict().renormalise()\n",
    "        real = gridpred.intensity_matrix.data\n",
    "        # Return the intensity matrix data\n",
    "        return real\n",
    "    except:\n",
    "        # If renormalisation fails or prediction is empty, return the intensity matrix directly\n",
    "        return predictor.predict().intensity_matrix\n",
    "\n",
    "# Import necessary libraries for Earth Mover's Distance calculation\n",
    "from scipy.optimize import linprog\n",
    "from math import sqrt\n",
    "\n",
    "# Define a function to calculate the Earth Mover's Distance (EMD) between two intensity matrices\n",
    "# Takes two intensity matrices (m1, m2) and the grid cell size (cuadricula) as input\n",
    "def EMD(m1, m2, cuadricula):\n",
    "    # Initialize lists to store points and their masses for the EMD calculation\n",
    "    P = [] # Points and masses for m1\n",
    "    Q = [] # Points and masses for m2\n",
    "    coordenadas = [] # List to store grid cell coordinates\n",
    "\n",
    "    # --- PART 1: Setup Points and Objective Function ---\n",
    "    # Iterate through the grid cells to get coordinates and masses (intensity values)\n",
    "    # Since m1 and m2 have the same size, one loop structure works for both\n",
    "    paso_filas = cuadricula / 2 # Starting y-coordinate for the center of the first row's cells\n",
    "    for i in range(0, m1.shape[0]):\n",
    "        paso_columna = cuadricula / 2 # Starting x-coordinate for the center of the first column's cells\n",
    "        for j in range(0, m1.shape[1]):\n",
    "            # Store the center coordinates of the current cell\n",
    "            coordenadas.append([paso_columna, paso_filas])\n",
    "            # Store the point (center coordinates) and its mass (intensity) for m1\n",
    "            P.append([[paso_columna, paso_filas], m1[i][j]])\n",
    "            # Store the point (center coordinates) and its mass (intensity) for m2\n",
    "            Q.append([[paso_columna, paso_filas], m2[i][j]])\n",
    "            # Move to the center of the next column's cell\n",
    "            paso_columna += cuadricula\n",
    "        # Move to the center of the next row's cells\n",
    "        paso_filas += cuadricula\n",
    "\n",
    "    # Initialize and generate the objective function coefficients (distances between all pairs of points)\n",
    "    # The objective is to minimize the total cost of moving 'earth' (mass) between distributions\n",
    "    obj = []\n",
    "    for i in range(0, len(P)):\n",
    "        for j in range(0, len(Q)):\n",
    "            # Calculate the Euclidean distance between point P[i][0] and point Q[j][0]\n",
    "            distance = sqrt(pow(P[i][0][0] - Q[j][0][0], 2) + pow(P[i][0][1] - Q[j][0][1], 2))\n",
    "            obj.append(distance)\n",
    "\n",
    "    # --- PART 2: Inequality Constraints ---\n",
    "    # These constraints ensure that the total flow of 'earth' out of each source point \n",
    "# is less than or equal to its mass, and the total flow into each sink point \n",
    "# is less than or equal to its capacity.\n",
    "    lhs_ineq = [] # Left-hand side of inequality constraints\n",
    "\n",
    "    # Constraints for row sums (flow out of source points P)\n",
    "    for m in range(0, len(P)):\n",
    "        # Create a row for the inequality matrix\n",
    "        aux = np.zeros((len(P), len(Q)))\n",
    "        # Set ones in the positions corresponding to flows from source point m\n",
    "        aux[m:m+1] = np.ones((1, len(Q)))\n",
    "        # Flatten the row and add it to the constraints list\n",
    "        lhs_ineq.append(np.asarray(aux).reshape(-1))\n",
    "\n",
    "    # Constraints for column sums (flow into sink points Q)\n",
    "    for m in range(0, len(Q)):\n",
    "        # Create a row for the inequality matrix\n",
    "        aux = np.zeros((len(P), len(Q)))\n",
    "        # Set ones in the positions corresponding to flows into sink point m\n",
    "        aux[:, m:m+1] = np.ones((len(P), 1))\n",
    "        # Flatten the row and add it to the constraints list\n",
    "        lhs_ineq.append(np.asarray(aux).reshape(-1))\n",
    "\n",
    "    # Right-hand side of inequality constraints (masses of points P and Q)\n",
    "    rhs_ineq = []\n",
    "    # Masses for source points P\n",
    "    for m in range(0, len(P)):\n",
    "        rhs_ineq.append(P[m][1])\n",
    "    # Masses for sink points Q\n",
    "    for m in range(0, len(Q)):\n",
    "        rhs_ineq.append(Q[m][1])\n",
    "\n",
    "    # --- PART 3: Equality Constraints ---\n",
    "    # This constraint ensures that the total flow is equal to the minimum of the total mass in P and Q.\n",
    "    lhs_eq = [[1 for i in range(0, len(P) * len(Q))]] # Left-hand side of the equality constraint (sum of all flows)\n",
    "    rhs_eq = [min(sum([i[1] for i in P]), sum([i[1] for i in Q]))] # Right-hand side (minimum total mass)\n",
    "\n",
    "    # --- PART 4: Bounds for Variables ---\n",
    "    # Define bounds for the flow variables f_ij (flow from point i in P to point j in Q)\n",
    "    # The flow must be non-negative and at most 1 (assuming normalized masses)\n",
    "    bnd = [(0, 1) for i in range(0, len(P) * len(Q))]\n",
    "\n",
    "    # --- PART 5: Solve Linear Programming Problem ---\n",
    "    # Solve the linear programming problem to find the optimal flow (x) that minimizes the total cost\n",
    "    # The method 'highs' is generally recommended for speed and reliability\n",
    "    # opt = linprog(c=obj, A_ub=lhs_ineq, b_ub=rhs_ineq, A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd, method=\"revised simplex\") # Original commented method\n",
    "    opt = linprog(c=obj, A_ub=lhs_ineq, b_ub=rhs_ineq, A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd, method=\"highs\")\n",
    "\n",
    "    # Reshape the solution array (optimal flows) back into a matrix format corresponding to P and Q\n",
    "    solucion = opt.x.reshape(len(P), len(Q))\n",
    "\n",
    "    # --- EMD CALCULATION ---\n",
    "    # Calculate the Earth Mover's Distance using the optimal flow (solucion) and the distances (obj)\n",
    "    emd = 0\n",
    "    # Iterate through all pairs of points (i from P, j from Q)\n",
    "    for i in range(0, len(P)):\n",
    "        for j in range(0, len(Q)):\n",
    "            # Add the product of the optimal flow between point i and point j and the distance between them\n",
    "            distance = sqrt(pow(P[i][0][0] - Q[j][0][0], 2) + pow(P[i][0][1] - Q[j][0][1], 2))\n",
    "            emd += (solucion[i][j]) * (distance)\n",
    "\n",
    "    # Normalize the EMD by the total flow (which should be min(sum(P_mass), sum(Q_mass)))\n",
    "    # Note: Summing 'solucion' twice seems incorrect. It should be normalized by the total mass moved.\n",
    "    # A common normalization is by the total mass, which is min(sum(P_mass), sum(Q_mass)).\n",
    "    # The code calculates solucion.sum().sum() which is just the sum of the flattened solution array.\n",
    "    # Assuming the normalization logic in the original code is intended:\n",
    "    emd = emd / solucion.sum().sum()\n",
    "\n",
    "    # Return the calculated EMD\n",
    "    return (emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SEPP Model Parameters\n",
    "\n",
    "This cell defines the parameters to be used for the SEPP model evaluation. These parameters, `hours` (temporal decay) and `cutoff` (spatial decay), influence how the SEPP model weights historical events when making predictions. The defined `params` dictionary is used in the evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0PBj02f9BPh"
   },
   "outputs": [],
   "source": [
    "# Commented-out code likely used for grid search or parameter tuning for models like SEPP\n",
    "\n",
    "### grid search\n",
    "\n",
    "\n",
    "### SEPP parameter exploration notes\n",
    "# Example parameters explored:\n",
    "# hours         1  12  24  24*5    24*15    24*30    24*90\n",
    "# cut_space     sqrt(2)       1         sqrt(2)/2     0.5     0.1\n",
    "\n",
    "# Define the specific parameters to be used for the SEPP model in this run\n",
    "# 'hours' likely relates to the time window for considering past events (e.g., 24*30*6 hours)\n",
    "# 'cutoff' likely relates to the spatial influence distance (e.g., grid_size)\n",
    "params={\n",
    "    \"hours\":[24*30*6],\n",
    "    \"cutoff\":[grid_size]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> <center> 6. Evaluate SEPP Model\n",
    "\n",
    "This major section is dedicated to training, predicting, and evaluating the SEPP predictive model across multiple data splits using the parameters defined in the previous cell. It iterates through each data split, trains the SEPP model, makes daily predictions, calculates the Earth Mover's Distance (EMD), and saves the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 SEPP Model Training and Evaluation Loop\n",
    "\n",
    "This code cell executes the main loop for the SEPP model evaluation. It manages loading data splits, training the SEPP model with specified parameters, performing daily predictions over the test period, calculating the EMD for each day, and saving the average EMD per data split to an Excel file. It also saves the daily predictions and real data grids for later inspection. The loops iterate through the data splits and the defined parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_8-53WKQ9BKa"
   },
   "outputs": [],
   "source": [
    "# Import necessary variables from global_vars\n",
    "from global_vars import dir_models, dir_split, f_final_train, grid_size, region # Ensure all needed global_vars are imported here or earlier\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm # tqdm was not used in the original SEPP loop, but kept import from prior cells\n",
    "# Import Colab-specific output clearing utility\n",
    "from google.colab import output # Ensure output is imported\n",
    "# Import datetime and timedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import pickle\n",
    "import pickle as pkl # Ensure pkl is imported\n",
    "# Import the EMD, intensity_m, and get_real_m helper functions\n",
    "# These functions should be defined in a preceding cell or imported from a module\n",
    "# Assuming they are defined in cell 4 based on notebook structure\n",
    "\n",
    "# Define the base path for saving SEPP model-related files\n",
    "path_models_sepp = os.path.join(dir_models, \"SEPP\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(path_models_sepp):\n",
    "    os.mkdir(path_models_sepp)\n",
    "\n",
    "# Define the path for the results table (Excel file)\n",
    "table_path = os.path.join(path_models_sepp, \"SEPP_EMD.xlsx\")\n",
    "\n",
    "# Check if the results table already exists\n",
    "if os.path.exists(table_path):\n",
    "    # If it exists, load the existing table\n",
    "    table = pd.read_excel(table_path)\n",
    "else:\n",
    "    # If not, create a new DataFrame with specified columns\n",
    "    table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "# Iterate through each data split (from 0 to 29)\n",
    "for id in range(0, 30):\n",
    "    # Clear previous output in environments like Colab\n",
    "    output.clear(wait=True)\n",
    "    # Print separator and current data ID\n",
    "    print(\"-\" * 30)\n",
    "    print(\"data:\", id)\n",
    "\n",
    "    # Check if results for this data ID already exist in the table\n",
    "    # Note: This check doesn't account for different parameter combinations.\n",
    "# If running multiple parameters, the table structure and check would need adjustment.\n",
    "    if len(table.loc[table[\"Data_id\"] == id]) > 0:\n",
    "        # If results exist, skip this iteration and print messages\n",
    "        print(\"Termino entrenamiento\")\n",
    "        print(\"Termino prediccion y medida EMD\")\n",
    "        continue\n",
    "\n",
    "    # Define the paths for the training and testing data files for the current data ID\n",
    "    # Uses dir_split from global_vars\n",
    "    train_path = os.path.join(dir_split, \"Train_Data_\" + str(id) + \".pkl\")\n",
    "    test_path = os.path.join(dir_split, \"Test_Data_\" + str(id) + \".pkl\")\n",
    "\n",
    "    # Load the training data from the pickle file\n",
    "    train = pkl.load(open(train_path, \"rb\"))\n",
    "    # Filter training data spatially to within the region [0, 1] x [0, 1]\n",
    "    train = train[(train.xcoords <= 1) & (train.xcoords >= 0) & ((train.ycoords <= 1)) & (train.ycoords >= 0)]\n",
    "\n",
    "    # Load the test data from the pickle file\n",
    "    test = pkl.load(open(test_path, \"rb\"))\n",
    "    # Filter test data spatially to within the region [0, 1] x [0, 1]\n",
    "    test = test[(test.xcoords <= 1) & (test.xcoords >= 0) & ((test.ycoords <= 1)) & (test.ycoords >= 0)]\n",
    "\n",
    "    # Define the directory path to save models and results specific to this data ID and model type\n",
    "    path_models_sepp_data = os.path.join(path_models_sepp, \"Data_\" + str(id))\n",
    "\n",
    "    # Create the data-specific directory if it doesn't exist\n",
    "    if not os.path.exists(path_models_sepp_data):\n",
    "        os.mkdir(path_models_sepp_data)\n",
    "\n",
    "    # Calculate the number of days in the test set\n",
    "    days_test = int((test.time_range[1] - test.time_range[0]).astype('timedelta64[D]') / np.timedelta64(1, 'D'))\n",
    "\n",
    "    # Iterate through the 'hours' parameters defined in the params dictionary\n",
    "    for times in params[\"hours\"]:\n",
    "        print(\"times:\", times)\n",
    "        # Iterate through the 'cutoff' parameters defined in the params dictionary\n",
    "        for spaces in params[\"cutoff\"]:\n",
    "            print(\"cutoff:\", spaces)\n",
    "\n",
    "            # Create a dictionary of the current parameter combination\n",
    "            prms = {\"hours\": times, \"cutoff\": spaces}\n",
    "\n",
    "            # Define the file path to save the trained SEPP model for the current data ID and parameters\n",
    "            # Note: The file name currently only includes data ID, not parameters. \n",
    "# Including parameters in the filename is recommended if params change within the loop.\n",
    "            path_file = os.path.join(path_models_sepp_data, \"Data_\" + str(id) + \".pkl\")\n",
    "\n",
    "            # Check if the trained model file already exists\n",
    "            if not os.path.exists(path_file):\n",
    "                # If the model doesn't exist, initialize and train the SEPP model\n",
    "                # The arguments are training data, grid size (20), hours, and cutoff\n",
    "                sepp_model_result = SEPP_MODEL(train, 20, times, spaces)\n",
    "                # Save the trained model to a pickle file\n",
    "                pkl.dump(sepp_model_result, open(path_file, \"wb\"))\n",
    "            else:\n",
    "                # If the model exists, load the trained model from the pickle file\n",
    "                sepp_model_result = pkl.load(open(path_file, \"rb\"))\n",
    "\n",
    "            # Print message indicating completion of training (or loading)\n",
    "            print(\"Termino entrenamiento\")\n",
    "\n",
    "            # Dictionaries to store daily predictions and real data grids\n",
    "            predictions = {}\n",
    "            reals = {}\n",
    "            # List to store EMD values for each day in the test set\n",
    "            EMD_mean = [] # Note: Variable name EMD_mean is used for a list of daily EMDs before averaging\n",
    "\n",
    "            # Iterate through each day in the test set to make predictions and calculate EMD\n",
    "            for i in range(days_test):\n",
    "                # Calculate the prediction date\n",
    "                pred_date = f_final_train + timedelta(days=i)\n",
    "\n",
    "                # Generate prediction for the current day using the trained SEPP model\n",
    "                # Original commented code suggests averaging predictions, but the active line does not.\n",
    "# predict = np.array([intensity_m(sepp_model_result,pred_date) for i in range(200)]).mean(axis=0)\n",
    "                predict = intensity_m(sepp_model_result, pred_date)\n",
    "                # Store the prediction for the current date\n",
    "                predictions[pred_date] = predict\n",
    "\n",
    "                # Get the real event intensity grid for the current date\n",
    "                real = get_real_m(test, pred_date, grid_size=grid_size, region=region) # Passing grid_size and region explicitly\n",
    "                # Store the real data grid for the current date\n",
    "                reals[pred_date] = real\n",
    "\n",
    "                # Calculate the EMD between the real and predicted grids\n",
    "                EMD_mean.append(EMD(real, predict, 1 / 25)) # Using 1/25 as grid size step\n",
    "\n",
    "            # Save the daily predictions and real data grids for the current data ID and parameters\n",
    "            # Note: File names do not include parameters. Recommended to include if params vary.\n",
    "            pkl.dump(predictions, open(os.path.join(path_models_sepp_data, \"predictions.pkl\"), \"wb\"))\n",
    "            pkl.dump(reals, open(os.path.join(path_models_sepp_data, \"reals.pkl\"), \"wb\"))\n",
    "\n",
    "            # Print message indicating completion of prediction and EMD calculation for this parameter set\n",
    "            print(\"Termino prediccion y medida EMD\")\n",
    "\n",
    "            # Calculate the mean EMD over all days in the test set for the current data ID and parameters\n",
    "            EMD_mean = np.array(EMD_mean).mean()\n",
    "\n",
    "            # Attempt to reload the results table (handles cases where another process might have updated it)\n",
    "            try:\n",
    "                table = pd.read_excel(table_path)\n",
    "            except:\n",
    "                # If reloading fails, re-initialize the table DataFrame\n",
    "                # Note: If params vary, the table structure might need columns for params.\n",
    "                table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "            # Add the results (Data ID and mean EMD) for the current data split and parameters to the table\n",
    "            # Note: This appends a row for each parameter combination if params has multiple.\n",
    "            table.loc[len(table)] = [id, EMD_mean]\n",
    "            # Save the updated results table to the Excel file\n",
    "            table.to_excel(table_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
