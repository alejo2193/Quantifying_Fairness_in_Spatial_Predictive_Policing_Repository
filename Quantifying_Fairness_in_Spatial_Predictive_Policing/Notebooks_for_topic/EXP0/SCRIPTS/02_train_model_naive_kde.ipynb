{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Notebook: Predictive Model Evaluation (NAIVE and KDE)\n",
    "\n",
    "## Purpose\n",
    "This notebook is designed to train and evaluate two predictive models, the NAIVE model and the Kernel Density Estimation (KDE) model, on spatio-temporal data splits. The primary goal is to assess their performance using the Earth Mover's Distance (EMD) metric.\n",
    "\n",
    "## Workflow Stage\n",
    "This notebook is in the **Model Training and Evaluation** stage of the data science workflow. It takes processed data splits, trains predictive models, generates predictions, and evaluates them against real data.\n",
    "\n",
    "## About\n",
    "This notebook processes multiple data splits to compare baseline prediction models. The results are saved for further analysis.\n",
    "\n",
    "**Author:** [Insert Author Name Here, e.g., Diego Alejandro Hernandez Castaneda]\n",
    "**Contact:** [Insert Contact Email Here]\n",
    "**Affiliation:** [Insert Affiliation Here]\n",
    "**Date:** [Insert Start Date] - [Insert End Date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "This section handles the initial setup, including mounting Google Drive (for environments like Colab), importing necessary system libraries, and configuring the Python path to include custom library directories. This is crucial for accessing project-specific modules and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20770,
     "status": "ok",
     "timestamp": 1696195703460,
     "user": {
      "displayName": "Diego Alejandro Hernandez Castaneda",
      "userId": "18178177893889256336"
     },
     "user_tz": 300
    },
    "id": "dU6EyFvoehee",
    "outputId": "7bbe4781-92be-41e9-f678-7a65be13e3e3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for environment setup\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "\n",
    "# Mount Google Drive. This is specific to Google Colab.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define base path for shared drive resources (replace with a variable if possible, or standardize)\n",
    "# Note: Hardcoding drive paths can reduce portability. Consider using relative paths or environment variables.\n",
    "# Example of standardized path base (requires defining a base directory variable, e.g., base_project_root):\n",
    "# base_project_root = '/path/to/your/project'\n",
    "# path_opencp = os.path.join(base_project_root, 'Librerias', 'PredictCode')\n",
    "# path_fairness = os.path.join(base_project_root, 'Librerias')\n",
    "# path_exp = os.path.join(base_project_root, 'Notebooks_for_topic', 'EXP0', 'SCRIPTS')\n",
    "\n",
    "# Original hardcoded paths - replace with standardized approach if possible\n",
    "path_opencp=\"drive/Shareddrives/FAIRNESS/Colab/Librerias/PredictCode/\"\n",
    "path_fairness=\"drive/Shareddrives/FAIRNESS/Colab/Librerias/\"\n",
    "path_exp=\"drive/Shareddrives/FAIRNESS/Colab/Notebooks_for_topic/EXP0/SCRIPTS/\"\n",
    "\n",
    "# Add custom library directories to the system path\n",
    "# This allows importing modules from these directories\n",
    "sys.path.insert(0, os.path.abspath(path_opencp))\n",
    "sys.path.insert(0, os.path.abspath(path_fairness))\n",
    "sys.path.insert(0, os.path.abspath(path_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Library Imports and Initial Configuration\n",
    "\n",
    "This cell imports the core libraries and custom modules required for the analysis, including data handling, numerical operations, date/time manipulations, the `open_cp` library for spatio-temporal analysis, and specific model implementations. It also changes the current working directory to the project's base experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12199,
     "status": "ok",
     "timestamp": 1696195715657,
     "user": {
      "displayName": "Diego Alejandro Hernandez Castaneda",
      "userId": "18178177893889256336"
     },
     "user_tz": 300
    },
    "id": "16cUdPzpnbBw",
    "outputId": "e56b50ea-0e57-42ab-c86c-3dccfb84fd4c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to load 'descartes' package.\n",
      "ERROR:open_cp.geometry:Failed to import `rtree`.\n",
      "ERROR:open_cp.network:Failed to import `rtree`.\n"
     ]
    }
   ],
   "source": [
    "# Import data handling and numerical libraries\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Import the open_cp library for spatio-temporal analysis\n",
    "import open_cp\n",
    "\n",
    "# Import Colab-specific output clearing utility\n",
    "from google.colab import output\n",
    "\n",
    "# Change the current working directory to the experiment base directory\n",
    "# Note: Hardcoding paths like this can reduce portability. \n",
    "# Consider defining a base directory variable (e.g., using os.path.join with a root path)\n",
    "# Example: os.chdir(os.path.join(base_project_root, 'Notebooks_for_topic', 'EXP0'))\n",
    "os.chdir(\"drive/Shareddrives/FAIRNESS/Colab/Notebooks_for_topic/EXP0/\")\n",
    "\n",
    "# Import specific predictive models from a custom module\n",
    "# These model classes are used for training and prediction\n",
    "from models.model_selection import NAIVE_MODEL, KDE_MODEL, SEPP_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXMoicv4nJhL"
   },
   "source": [
    "## 3. Variable and Region Setup\n",
    "\n",
    "This section sets up key variables required for the analysis, including the spatial region of interest and parameters imported from a `global_vars` file. This centralizes configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alULHi4WnPOr"
   },
   "outputs": [],
   "source": [
    "# Import necessary functions and variables from custom modules\n",
    "from open_cp.sources.sepp import make_time_unit\n",
    "\n",
    "# Import global variables for region definition and grid size\n",
    "# These variables are expected to be defined in global_vars.py\n",
    "from global_vars import x_min,x_max,y_min,y_max,grid_size,days_time_unit\n",
    "\n",
    "# Define the rectangular region of interest based on imported bounds\n",
    "region = open_cp.RectangularRegion(x_min,x_max, y_min,y_max)\n",
    "\n",
    "# Commented out parameters (cut_time, cut_space) - potentially for other models or experiments\n",
    "# cut_time=24*15\n",
    "# cut_space=0.5\n",
    "\n",
    "# Import more global variables for directory paths and final training date\n",
    "from global_vars import dir_sims, dir_split, f_final_train\n",
    "\n",
    "# Re-import model classes (redundant if already imported, but kept as in original)\n",
    "from models.model_selection import NAIVE_MODEL, KDE_MODEL, SEPP_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Prediction and Evaluation\n",
    "\n",
    "This cell defines essential helper functions used throughout the notebook to process model predictions, obtain real data in a comparable grid format, and calculate the Earth Mover's Distance (EMD) between predicted and real intensity matrices. These functions encapsulate key logic for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viiSJdVTuppu"
   },
   "outputs": [],
   "source": [
    "# Define a function to get the intensity matrix from a model prediction\n",
    "# Takes a model object and a prediction date as input\n",
    "def intensity_m(model, date, region=region, grid_size=grid_size):\n",
    "    # Attempt to get the prediction for the given date\n",
    "    try:\n",
    "        predict = model.predict(date)\n",
    "    except:\n",
    "        # If predicting requires no date argument, call predict() without it\n",
    "        predict = model.predict()\n",
    "\n",
    "    # Attempt to convert the prediction to a GridPredictionArray and get the intensity matrix\n",
    "    try:\n",
    "        M = open_cp.predictors.GridPredictionArray.from_continuous_prediction_region(predict, region, grid_size, grid_size)\n",
    "        intensity_matrix = M.intensity_matrix\n",
    "    except:\n",
    "        # If the prediction object already has an intensity matrix, use it\n",
    "        intensity_matrix = predict.intensity_matrix\n",
    "\n",
    "    # Check if the sum of the intensity matrix is zero\n",
    "    if intensity_matrix.sum() == 0:\n",
    "        # If sum is zero, return the matrix as is (no intensity)\n",
    "        return intensity_matrix\n",
    "    else:\n",
    "        # If sum is not zero, return the matrix normalized so the sum is 1\n",
    "        return intensity_matrix / intensity_matrix.sum()\n",
    "\n",
    "# Import necessary library for naive counting grid kernel\n",
    "import open_cp.naive as naive\n",
    "\n",
    "# Define a function to get the real event intensity matrix for a given date\n",
    "# Takes timed event points, a prediction date, grid size, and region as input\n",
    "def get_real_m(timedpoints, p_date, grid_size=grid_size, region=region):\n",
    "    # Filter events that occurred within the 24-hour period starting from p_date\n",
    "    real = timedpoints[(timedpoints.times_datetime() >= p_date) & ((timedpoints.times_datetime() < p_date + timedelta(days=1)))]\n",
    "\n",
    "    # Create a CountingGridKernel predictor with the specified grid size and region\n",
    "    predictor = naive.CountingGridKernel(grid_size, region=region)\n",
    "\n",
    "    # Set the filtered real events as the data for the predictor\n",
    "    predictor.data = real\n",
    "\n",
    "    # Attempt to get the grid prediction and renormalize it\n",
    "    try:\n",
    "        gridpred = predictor.predict().renormalise()\n",
    "        real = gridpred.intensity_matrix.data\n",
    "        # Return the intensity matrix data\n",
    "        return real\n",
    "    except:\n",
    "        # If renormalisation fails or prediction is empty, return the intensity matrix directly\n",
    "        return predictor.predict().intensity_matrix\n",
    "\n",
    "# Import necessary libraries for Earth Mover's Distance calculation\n",
    "from scipy.optimize import linprog\n",
    "from math import sqrt\n",
    "\n",
    "# Define a function to calculate the Earth Mover's Distance (EMD) between two intensity matrices\n",
    "# Takes two intensity matrices (m1, m2) and the grid cell size (cuadricula) as input\n",
    "def EMD(m1, m2, cuadricula):\n",
    "    # Initialize lists to store points and their masses for the EMD calculation\n",
    "    P = [] # Points and masses for m1\n",
    "    Q = [] # Points and masses for m2\n",
    "    coordenadas = [] # List to store grid cell coordinates\n",
    "\n",
    "    # --- PART 1: Setup Points and Objective Function ---\n",
    "    # Iterate through the grid cells to get coordinates and masses (intensity values)\n",
    "    # Since m1 and m2 have the same size, one loop structure works for both\n",
    "    paso_filas = cuadricula / 2 # Starting y-coordinate for the center of the first row's cells\n",
    "    for i in range(0, m1.shape[0]):\n",
    "        paso_columna = cuadricula / 2 # Starting x-coordinate for the center of the first column's cells\n",
    "        for j in range(0, m1.shape[1]):\n",
    "            # Store the center coordinates of the current cell\n",
    "            coordenadas.append([paso_columna, paso_filas])\n",
    "            # Store the point (center coordinates) and its mass (intensity) for m1\n",
    "            P.append([[paso_columna, paso_filas], m1[i][j]])\n",
    "            # Store the point (center coordinates) and its mass (intensity) for m2\n",
    "            Q.append([[paso_columna, paso_filas], m2[i][j]])\n",
    "            # Move to the center of the next column's cell\n",
    "            paso_columna += cuadricula\n",
    "        # Move to the center of the next row's cells\n",
    "        paso_filas += cuadricula\n",
    "\n",
    "    # Initialize and generate the objective function coefficients (distances between all pairs of points)\n",
    "    # The objective is to minimize the total cost of moving 'earth' (mass) between distributions\n",
    "    obj = []\n",
    "    for i in range(0, len(P)):\n",
    "        for j in range(0, len(Q)):\n",
    "            # Calculate the Euclidean distance between point P[i][0] and point Q[j][0]\n",
    "            distance = sqrt(pow(P[i][0][0] - Q[j][0][0], 2) + pow(P[i][0][1] - Q[j][0][1], 2))\n",
    "            obj.append(distance)\n",
    "\n",
    "    # --- PART 2: Inequality Constraints ---\n",
    "    # These constraints ensure that the total flow of 'earth' out of each source point \n",
    "# is less than or equal to its mass, and the total flow into each sink point \n",
    "# is less than or equal to its capacity.\n",
    "    lhs_ineq = [] # Left-hand side of inequality constraints\n",
    "\n",
    "    # Constraints for row sums (flow out of source points P)\n",
    "    for m in range(0, len(P)):\n",
    "        # Create a row for the inequality matrix\n",
    "        aux = np.zeros((len(P), len(Q)))\n",
    "        # Set ones in the positions corresponding to flows from source point m\n",
    "        aux[m:m+1] = np.ones((1, len(Q)))\n",
    "        # Flatten the row and add it to the constraints list\n",
    "        lhs_ineq.append(np.asarray(aux).reshape(-1))\n",
    "\n",
    "    # Constraints for column sums (flow into sink points Q)\n",
    "    for m in range(0, len(Q)):\n",
    "        # Create a row for the inequality matrix\n",
    "        aux = np.zeros((len(P), len(Q)))\n",
    "        # Set ones in the positions corresponding to flows into sink point m\n",
    "        aux[:, m:m+1] = np.ones((len(P), 1))\n",
    "        # Flatten the row and add it to the constraints list\n",
    "        lhs_ineq.append(np.asarray(aux).reshape(-1))\n",
    "\n",
    "    # Right-hand side of inequality constraints (masses of points P and Q)\n",
    "    rhs_ineq = []\n",
    "    # Masses for source points P\n",
    "    for m in range(0, len(P)):\n",
    "        rhs_ineq.append(P[m][1])\n",
    "    # Masses for sink points Q\n",
    "    for m in range(0, len(Q)):\n",
    "        rhs_ineq.append(Q[m][1])\n",
    "\n",
    "    # --- PART 3: Equality Constraints ---\n",
    "    # This constraint ensures that the total flow is equal to the minimum of the total mass in P and Q.\n",
    "    lhs_eq = [[1 for i in range(0, len(P) * len(Q))]] # Left-hand side of the equality constraint (sum of all flows)\n",
    "    rhs_eq = [min(sum([i[1] for i in P]), sum([i[1] for i in Q]))] # Right-hand side (minimum total mass)\n",
    "\n",
    "    # --- PART 4: Bounds for Variables ---\n",
    "    # Define bounds for the flow variables f_ij (flow from point i in P to point j in Q)\n",
    "    # The flow must be non-negative and at most 1 (assuming normalized masses)\n",
    "    bnd = [(0, 1) for i in range(0, len(P) * len(Q))]\n",
    "\n",
    "    # --- PART 5: Solve Linear Programming Problem ---\n",
    "    # Solve the linear programming problem to find the optimal flow (x) that minimizes the total cost\n",
    "    # The method 'highs' is generally recommended for speed and reliability\n",
    "    # opt = linprog(c=obj, A_ub=lhs_ineq, b_ub=rhs_ineq, A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd, method=\"revised simplex\") # Original commented method\n",
    "    opt = linprog(c=obj, A_ub=lhs_ineq, b_ub=rhs_ineq, A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd, method=\"highs\")\n",
    "\n",
    "    # Reshape the solution array (optimal flows) back into a matrix format corresponding to P and Q\n",
    "    solucion = opt.x.reshape(len(P), len(Q))\n",
    "\n",
    "    # --- EMD CALCULATION ---\n",
    "    # Calculate the Earth Mover's Distance using the optimal flow (solucion) and the distances (obj)\n",
    "    emd = 0\n",
    "    # Iterate through all pairs of points (i from P, j from Q)\n",
    "    for i in range(0, len(P)):\n",
    "        for j in range(0, len(Q)):\n",
    "            # Add the product of the optimal flow between point i and point j and the distance between them\n",
    "            distance = sqrt(pow(P[i][0][0] - Q[j][0][0], 2) + pow(P[i][0][1] - Q[j][0][1], 2))\n",
    "            emd += (solucion[i][j]) * (distance)\n",
    "\n",
    "    # Normalize the EMD by the total flow (which should be min(sum(P_mass), sum(Q_mass)))\n",
    "    # Note: Summing 'solucion' twice seems incorrect. It should be normalized by the total mass moved.\n",
    "    # A common normalization is by the total mass, which is min(sum(P_mass), sum(Q_mass)).\n",
    "    # The code calculates solucion.sum().sum() which is just the sum of the flattened solution array.\n",
    "    # Assuming the normalization logic in the original code is intended:\n",
    "    emd = emd / solucion.sum().sum()\n",
    "\n",
    "    # Return the calculated EMD\n",
    "    return (emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Exploration (Commented Out)\n",
    "\n",
    "This cell contains commented-out code that appears to be related to parameter exploration or grid search for predictive models, particularly for the SEPP model. It is kept here as part of the notebook's history but is not currently active in the execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0PBj02f9BPh"
   },
   "outputs": [],
   "source": [
    "# Commented-out code likely used for grid search or parameter tuning for models like SEPP\n",
    "\n",
    "### grid search\n",
    "\n",
    "\n",
    "### SEPP\n",
    "\n",
    "## hours         1  12  24  24*5    24*15    24*30    24*90\n",
    "## cut_space     sqrt(2)       1         sqrt(2)/2     0.5     0.1\n",
    "\n",
    "#params={\n",
    "#   \"hours\":[1,12,24,24*5,24*30,24*90],\n",
    "#   \"cutoff\":[sqrt(2),1,sqrt(2)/2,0.5,0.1]\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading and GeoJSON Export (Example)\n",
    "\n",
    "This cell demonstrates how to load a specific data split (likely test data based on the filename), filter it spatially, convert it to a GeoDataFrame, and export it as a GeoJSON file. This is useful for visualizing the spatial distribution of events."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import geopandas for handling geospatial data and shapely for geometric objects\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Define the path to the data split file using os.path.join for portability\n",
    "# It uses dir_split from global_vars and hardcoded file name components\n",
    "# Consider making the data split ID (15) a variable for easier iteration/testing\n",
    "data_split_id = 15\n",
    "train_path = os.path.join(dir_split, \"Test_Data_\" + str(data_split_id) + \".pkl\")\n",
    "\n",
    "# Load the data from the pickle file\n",
    "train = pkl.load(open(train_path, \"rb\"))\n",
    "\n",
    "# Filter the data spatially to within the region [0, 1] x [0, 1]\n",
    "train = train[(train.xcoords <= 1) & (train.xcoords >= 0) & ((train.ycoords <= 1)) & (train.ycoords >= 0)]\n",
    "\n",
    "# Create a list of shapely Point objects from the x and y coordinates\n",
    "geometry = [Point(xy) for xy in zip(train.xcoords, train.ycoords)]\n",
    "\n",
    "# Create a GeoDataFrame from the geometry objects\n",
    "train_data = gpd.GeoDataFrame({'geometry': geometry})\n",
    "\n",
    "# Define the output path for the GeoJSON file\n",
    "# Note: Hardcoded 'DATOS/' path can be standardized using os.path.join with an output directory variable\n",
    "# Example: output_data_dir = '/path/to/output/data'\n",
    "# output_geojson_path = os.path.join(output_data_dir, 'tain_example_test.geojson')\n",
    "output_geojson_path = 'DATOS/tain_example_test.geojson'\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "# Ensure the output directory exists before saving\n",
    "output_dir = os.path.dirname(output_geojson_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_data.to_file(output_geojson_path, driver='GeoJSON')"
   ],
   "metadata": {
    "id": "ouEZI12iJPSb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Generation and GeoJSON Export\n",
    "\n",
    "This cell generates a grid of rectangular geometries covering a defined area (0 to 1 in both x and y) with a specified number of rows and columns. This grid can be used for visualization or spatial analysis purposes, and it is exported as a GeoJSON file."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import shapely geometry module for creating geometric shapes\n",
    "import shapely.geometry as geom\n",
    "\n",
    "# Define the coordinates for the overall region bounding box\n",
    "xmin = 0\n",
    "ymin = 0\n",
    "xmax = 1\n",
    "ymax = 1\n",
    "\n",
    "# Create an empty list to store the cell geometries\n",
    "geometries = []\n",
    "\n",
    "# Define the grid dimensions (rows and columns)\n",
    "# Note: These are hardcoded; consider making them variables, potentially from global_vars or parameters.\n",
    "rows = 5\n",
    "cols = 5\n",
    "\n",
    "# Calculate the width and height of each grid cell\n",
    "cell_width = (xmax - xmin) / cols\n",
    "cell_height = (ymax - ymin) / rows\n",
    "\n",
    "# Generate the geometries for each grid cell\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        # Calculate the coordinates of the bottom-left corner of the cell\n",
    "        x1 = xmin + j * cell_width\n",
    "        y1 = ymin + i * cell_height\n",
    "        # Calculate the coordinates of the top-right corner of the cell\n",
    "        x2 = x1 + cell_width\n",
    "        y2 = y1 + cell_height\n",
    "        # Create a rectangular polygon (box) for the current cell\n",
    "        cell = geom.box(x1, y1, x2, y2)\n",
    "        # Add the cell geometry to the list\n",
    "        geometries.append(cell)\n",
    "\n",
    "# Create a GeoDataFrame from the list of geometries\n",
    "gdf = gpd.GeoDataFrame({'geometry': geometries})\n",
    "\n",
    "# Define the output path for the GeoJSON file\n",
    "# Note: Hardcoded 'DATOS/' path can be standardized using os.path.join with an output directory variable\n",
    "# Example: output_data_dir = '/path/to/output/data'\n",
    "# output_geojson_path = os.path.join(output_data_dir, 'cuadricula.geojson')\n",
    "output_geojson_path = 'DATOS/cuadricula.geojson'\n",
    "\n",
    "# Ensure the output directory exists before saving\n",
    "output_dir = os.path.dirname(output_geojson_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the GeoDataFrame as a GeoJSON file\n",
    "gdf.to_file(output_geojson_path, driver='GeoJSON')"
   ],
   "metadata": {
    "id": "6BO5q71MKKQu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tHv5HqTwxM0"
   },
   "source": [
    "# <font color='red'> <center> 8. Evaluate NAIVE Model\n",
    "\n",
    "This major section is dedicated to training, predicting, and evaluating the NAIVE predictive model across multiple data splits. It iterates through each data split, trains a separate model for each day of the week, makes daily predictions, calculates the Earth Mover's Distance (EMD), and saves the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 NAIVE Model Training and Evaluation Loop\n",
    "\n",
    "This code cell executes the main loop for the NAIVE model evaluation. It manages loading data splits, training day-specific models, performing daily predictions over the test period, calculating the EMD for each day, and saving the average EMD per data split to an Excel file. It also saves the daily predictions and real data grids for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1693710092776,
     "user": {
      "displayName": "Cristian Alejandro Pulido Quintero",
      "userId": "14005360977426477184"
     },
     "user_tz": 300
    },
    "id": "_8-53WKQ9BKa",
    "outputId": "5c182979-4a56-4ec5-82ad-9d06266cbc3b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r100%|██████████| 30/30 [00:00<00:00, 117.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------\n",
      "data: 29\n",
      "Termino entrenamiento\n",
      "Termino prediccion y medida EMD\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import directory path for saving models from global_vars\n",
    "from global_vars import dir_models, dir_split, f_final_train, grid_size, region # Ensure all needed global_vars are imported here or earlier\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the base path for saving NAIVE model-related files\n",
    "path_models_naive = os.path.join(dir_models, \"NAIVE\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(path_models_naive):\n",
    "    os.mkdir(path_models_naive)\n",
    "\n",
    "# Define the path for the results table (Excel file)\n",
    "table_path = os.path.join(path_models_naive, \"NAIVE_EMD.xlsx\")\n",
    "\n",
    "# Check if the results table already exists\n",
    "if os.path.exists(table_path):\n",
    "    # If it exists, load the existing table\n",
    "    table = pd.read_excel(table_path)\n",
    "else:\n",
    "    # If not, create a new DataFrame with specified columns\n",
    "    table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "# Iterate through each data split (from 0 to 29)\n",
    "# tqdm provides a progress bar for the loop\n",
    "for id in tqdm(range(30)):\n",
    "    # Clear previous output in environments like Colab\n",
    "    output.clear(wait=True)\n",
    "    # Print separator and current data ID\n",
    "    print(\"-\" * 30)\n",
    "    print(\"data:\", id)\n",
    "\n",
    "    # Check if results for this data ID already exist in the table\n",
    "    if len(table.loc[table[\"Data_id\"] == id]) > 0:\n",
    "        # If results exist, skip this iteration and print messages\n",
    "        print(\"Termino entrenamiento\")\n",
    "        print(\"Termino prediccion y medida EMD\")\n",
    "        continue\n",
    "\n",
    "    # Define the paths for the training and testing data files for the current data ID\n",
    "    # Uses dir_split from global_vars\n",
    "    train_path = os.path.join(dir_split, \"Train_Data_\" + str(id) + \".pkl\")\n",
    "    test_path = os.path.join(dir_split, \"Test_Data_\" + str(id) + \".pkl\")\n",
    "\n",
    "    # Load the training data from the pickle file\n",
    "    train = pkl.load(open(train_path, \"rb\"))\n",
    "    # Filter training data spatially to within the region [0, 1] x [0, 1]\n",
    "    train = train[(train.xcoords <= 1) & (train.xcoords >= 0) & ((train.ycoords <= 1)) & (train.ycoords >= 0)]\n",
    "\n",
    "    # Load the test data from the pickle file\n",
    "    test = pkl.load(open(test_path, \"rb\"))\n",
    "    # Filter test data spatially to within the region [0, 1] x [0, 1]\n",
    "    test = test[(test.xcoords <= 1) & (test.xcoords >= 0) & ((test.ycoords <= 1)) & (test.ycoords >= 0)]\n",
    "\n",
    "    # Define the directory path to save models specific to this data ID\n",
    "    path_models_naive_data = os.path.join(path_models_naive, \"Data_\" + str(id))\n",
    "\n",
    "    # Create the data-specific models directory if it doesn't exist\n",
    "    if not os.path.exists(path_models_naive_data):\n",
    "        os.mkdir(path_models_naive_data)\n",
    "\n",
    "    # Calculate the number of days in the test set\n",
    "    days_test = int((test.time_range[1] - test.time_range[0]).astype('timedelta64[D]') / np.timedelta64(1, 'D'))\n",
    "\n",
    "    # Dictionary to store trained models, one for each day of the week\n",
    "    models_by_day = {}\n",
    "\n",
    "    # Train a separate NAIVE model for each day of the week (0=Monday to 6=Sunday)\n",
    "    for day in range(7):\n",
    "        # Define the file path to save the model for the current day\n",
    "        path_file = os.path.join(path_models_naive_data, \"Data_\" + str(id) + \"_day_\" + str(day) + \".pkl\")\n",
    "\n",
    "        # Check if the model file for this day already exists\n",
    "        if not os.path.exists(path_file):\n",
    "            # If the model doesn't exist, filter training data for the current day of the week\n",
    "            train_filter = train[[i[0].astype(datetime.datetime).weekday() == day for i in train]]\n",
    "            # Initialize and train the NAIVE model with the filtered data\n",
    "            models_by_day[day] = NAIVE_MODEL(train_filter)\n",
    "            # Save the trained model to a pickle file\n",
    "            pkl.dump(models_by_day[day], open(path_file, \"wb\"))\n",
    "        else:\n",
    "            # If the model exists, load the trained model from the pickle file\n",
    "            models_by_day[day] = pkl.load(open(path_file, \"rb\"))\n",
    "\n",
    "        # Print message indicating completion of training for the current day\n",
    "        print(\"Termino entrenamiento day \", str(day))\n",
    "\n",
    "    # Dictionaries to store daily predictions and real data grids\n",
    "    predictions = {}\n",
    "    reals = {}\n",
    "    # List to store EMD values for each day in the test set\n",
    "    EMD_mean = [] # Note: Variable name EMD_mean seems misleading as it's a list of daily EMDs before averaging\n",
    "\n",
    "    # Iterate through each day in the test set\n",
    "    for i in tqdm(range(days_test)):\n",
    "        # Clear previous output\n",
    "        output.clear(wait=True)\n",
    "        # Calculate the prediction date\n",
    "        pred_date = f_final_train + timedelta(days=i)\n",
    "        # Get the day of the week for the prediction date\n",
    "        day_filter = pred_date.weekday()\n",
    "\n",
    "        # Generate predictions for the current day using the trained model for that day of the week\n",
    "        # The prediction is averaged over 100 repetitions (purpose unclear from code - potentially for smoothing or stability)\n",
    "        predict = np.array([intensity_m(models_by_day[day_filter], pred_date) for i in range(100)]).mean(axis=0)\n",
    "        # Store the prediction for the current date\n",
    "        predictions[pred_date] = predict\n",
    "\n",
    "        # Get the real event intensity grid for the current date\n",
    "        real = get_real_m(test, pred_date, grid_size=grid_size, region=region) # Passing grid_size and region explicitly for clarity\n",
    "        # Store the real data grid for the current date\n",
    "        reals[pred_date] = real\n",
    "\n",
    "        # Calculate the EMD between the real and predicted grids\n",
    "        try:\n",
    "            # Append the calculated EMD for the day to the list\n",
    "            EMD_mean.append(EMD(real, predict, 1 / 25)) # Using 1/25 as grid size step as seen in EMD function\n",
    "        except:\n",
    "            # If EMD calculation fails, skip this day\n",
    "            continue\n",
    "\n",
    "    # Calculate the mean EMD over all days in the test set for the current data ID\n",
    "    # Note: This calculation is inside the inner loop; it should be outside after the loop finishes.\n",
    "    # Corrected placement would be after the 'for i in tqdm(range(days_test)):' loop\n",
    "    # Corrected line (assuming intended behavior is mean over days): EMD_mean_for_id = np.array(EMD_mean).mean()\n",
    "    # Keeping original placement and variable name as per instruction not to change logic/structure\n",
    "    EMD_mean = np.array(EMD_mean).mean()\n",
    "\n",
    "    # Save the daily predictions and real data grids for the current data ID\n",
    "    pkl.dump(predictions, open(os.path.join(path_models_naive_data, \"predictions.pkl\"), \"wb\"))\n",
    "    pkl.dump(reals, open(os.path.join(path_models_naive_data, \"reals.pkl\"), \"wb\"))\n",
    "\n",
    "    # Print message indicating completion of prediction and EMD calculation\n",
    "    print(\"Termino prediccion y medida EMD\")\n",
    "\n",
    "    # Attempt to reload the results table (handles cases where another process might have updated it)\n",
    "    try:\n",
    "        table = pd.read_excel(table_path)\n",
    "    except:\n",
    "        # If reloading fails, re-initialize the table DataFrame\n",
    "        table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "    # Add the results (Data ID and mean EMD) for the current data split to the table\n",
    "    table.loc[len(table)] = [id, EMD_mean]\n",
    "    # Save the updated results table to the Excel file\n",
    "    table.to_excel(table_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owqr70JLwmgE"
   },
   "source": [
    "# <font color='red'> <center> 9. Evaluate KDE Model\n",
    "\n",
    "This major section mirrors the structure of the NAIVE model evaluation but applies it to the Kernel Density Estimation (KDE) model. It performs training, prediction, and evaluation for the KDE model across the same data splits, calculating and saving the EMD results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 KDE Model Training and Evaluation Loop\n",
    "\n",
    "This code cell executes the main loop for the KDE model evaluation. It follows the same steps as the NAIVE evaluation: loading data, training day-specific KDE models (with a specified time kernel), performing daily predictions, calculating EMD, and saving the mean EMD per data split, along with daily predictions and real data grids."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the KDE module from open_cp\n",
    "import open_cp.kde as kde\n",
    "\n",
    "# Import necessary variables from global_vars\n",
    "from global_vars import dir_models, dir_split, f_final_train, grid_size, region # Ensure all needed global_vars are imported\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "# Import Colab-specific output clearing utility\n",
    "from google.colab import output # Ensure output is imported\n",
    "# Import datetime and timedelta\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base path for saving KDE model-related files\n",
    "path_models_kde = os.path.join(dir_models, \"KDE\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(path_models_kde):\n",
    "    os.mkdir(path_models_kde)\n",
    "\n",
    "# Define the path for the results table (Excel file)\n",
    "table_path = os.path.join(path_models_kde, \"KDE_EMD.xlsx\")\n",
    "\n",
    "# Check if the results table already exists\n",
    "if os.path.exists(table_path):\n",
    "    # If it exists, load the existing table\n",
    "    table = pd.read_excel(table_path)\n",
    "else:\n",
    "    # If not, create a new DataFrame with specified columns\n",
    "    table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "# Iterate through each data split (from 0 to 29)\n",
    "# tqdm provides a progress bar for the loop\n",
    "for id in tqdm(range(30)):\n",
    "    # Clear previous output in environments like Colab\n",
    "    output.clear(wait=True)\n",
    "    # Print separator and current data ID\n",
    "    print(\"-\" * 30)\n",
    "    print(\"data:\", id)\n",
    "\n",
    "    # Check if results for this data ID already exist in the table\n",
    "    if len(table.loc[table[\"Data_id\"] == id]) > 0:\n",
    "        # If results exist, skip this iteration and print messages\n",
    "        print(\"Termino entrenamiento\")\n",
    "        print(\"Termino prediccion y medida EMD\")\n",
    "        continue\n",
    "\n",
    "    # Define the paths for the training and testing data files for the current data ID\n",
    "    # Uses dir_split from global_vars\n",
    "    train_path = os.path.join(dir_split, \"Train_Data_\" + str(id) + \".pkl\")\n",
    "    test_path = os.path.join(dir_split, \"Test_Data_\" + str(id) + \".pkl\")\n",
    "\n",
    "    # Load the training data from the pickle file\n",
    "    train = pkl.load(open(train_path, \"rb\"))\n",
    "    # Filter training data spatially to within the region [0, 1] x [0, 1]\n",
    "    train = train[(train.xcoords <= 1) & (train.xcoords >= 0) & ((train.ycoords <= 1)) & (train.ycoords >= 0)]\n",
    "\n",
    "    # Load the test data from the pickle file\n",
    "    test = pkl.load(open(test_path, \"rb\"))\n",
    "    # Filter test data spatially to within the region [0, 1] x [0, 1]\n",
    "    test = test[(test.xcoords <= 1) & (test.xcoords >= 0) & ((test.ycoords <= 1)) & (test.ycoords >= 0)]\n",
    "\n",
    "    # Define the directory path to save models specific to this data ID\n",
    "    path_models_kde_data = os.path.join(path_models_kde, \"Data_\" + str(id))\n",
    "\n",
    "    # Create the data-specific models directory if it doesn't exist\n",
    "    if not os.path.exists(path_models_kde_data):\n",
    "        os.mkdir(path_models_kde_data)\n",
    "\n",
    "    # Calculate the number of days in the test set\n",
    "    days_test = int((test.time_range[1] - test.time_range[0]).astype('timedelta64[D]') / np.timedelta64(1, 'D'))\n",
    "\n",
    "    # Dictionary to store trained models, one for each day of the week\n",
    "    models_by_day = {}\n",
    "\n",
    "    # Train a separate KDE model for each day of the week (0=Monday to 6=Sunday)\n",
    "    for day in range(7):\n",
    "        # Define the file path to save the model for the current day\n",
    "        path_file = os.path.join(path_models_kde_data, \"Data_\" + str(id) + \"_day_\" + str(day) + \".pkl\")\n",
    "\n",
    "        # Check if the model file for this day already exists\n",
    "        if not os.path.exists(path_file):\n",
    "            # If the model doesn't exist, filter training data for the current day of the week\n",
    "            train_filter = train[[i[0].astype(datetime.datetime).weekday() == day for i in train]]\n",
    "            # Initialize and train the KDE model with the filtered data, region, grid size, and an ExponentialTimeKernel\n",
    "            # The ExponentialTimeKernel(7) uses a decay parameter of 7\n",
    "            models_by_day[day] = KDE_MODEL(train_filter, region, grid_size, kde.ExponentialTimeKernel(7))\n",
    "            # Save the trained model to a pickle file\n",
    "            pkl.dump(models_by_day[day], open(path_file, \"wb\"))\n",
    "        else:\n",
    "            # If the model exists, load the trained model from the pickle file\n",
    "            models_by_day[day] = pkl.load(open(path_file, \"rb\"))\n",
    "\n",
    "        # Print message indicating completion of training for the current day\n",
    "        print(\"Termino entrenamiento day \", str(day))\n",
    "\n",
    "    # Dictionaries to store daily predictions and real data grids\n",
    "    predictions = {}\n",
    "    reals = {}\n",
    "    # List to store EMD values for each day in the test set\n",
    "    EMD_mean = [] # Note: Variable name EMD_mean seems misleading as it's a list of daily EMDs before averaging\n",
    "\n",
    "    # Iterate through each day in the test set\n",
    "    for i in tqdm(range(days_test)):\n",
    "        # Clear previous output\n",
    "        output.clear(wait=True)\n",
    "        # Calculate the prediction date\n",
    "        pred_date = f_final_train + timedelta(days=i)\n",
    "        # Get the day of the week for the prediction date\n",
    "        day_filter = pred_date.weekday()\n",
    "\n",
    "        # Generate predictions for the current day using the trained model for that day of the week\n",
    "        # The prediction is averaged over 100 repetitions (purpose unclear from code)\n",
    "        predict = np.array([intensity_m(models_by_day[day_filter], pred_date) for i in range(100)]).mean(axis=0)\n",
    "        # Store the prediction for the current date\n",
    "        predictions[pred_date] = predict\n",
    "\n",
    "        # Get the real event intensity grid for the current date\n",
    "        real = get_real_m(test, pred_date, grid_size=grid_size, region=region) # Passing grid_size and region explicitly\n",
    "        # Store the real data grid for the current date\n",
    "        reals[pred_date] = real\n",
    "\n",
    "        # Calculate the EMD between the real and predicted grids\n",
    "        try:\n",
    "            # Append the calculated EMD for the day to the list\n",
    "            EMD_mean.append(EMD(real, predict, 1 / 25)) # Using 1/25 as grid size step\n",
    "        except:\n",
    "            # If EMD calculation fails, skip this day\n",
    "            continue\n",
    "\n",
    "    # Calculate the mean EMD over all days in the test set for the current data ID\n",
    "    # Note: This calculation is inside the inner loop; it should be outside after the loop finishes.\n",
    "    # Corrected line (assuming intended behavior is mean over days): EMD_mean_for_id = np.array(EMD_mean).mean()\n",
    "    # Keeping original placement and variable name\n",
    "    EMD_mean = np.array(EMD_mean).mean()\n",
    "\n",
    "    # Save the daily predictions and real data grids for the current data ID\n",
    "    pkl.dump(predictions, open(os.path.join(path_models_kde_data, \"predictions.pkl\"), \"wb\"))\n",
    "    pkl.dump(reals, open(os.path.join(path_models_kde_data, \"reals.pkl\"), \"wb\"))\n",
    "\n",
    "    # Print message indicating completion of prediction and EMD calculation\n",
    "    print(\"Termino prediccion y medida EMD\")\n",
    "\n",
    "    # Attempt to reload the results table\n",
    "    try:\n",
    "        table = pd.read_excel(table_path)\n",
    "    except:\n",
    "        # If reloading fails, re-initialize the table DataFrame\n",
    "        table = pd.DataFrame(columns=[\"Data_id\", \"EMD\"])\n",
    "\n",
    "    # Add the results for the current data split to the table\n",
    "    table.loc[len(table)] = [id, EMD_mean]\n",
    "    # Save the updated results table to the Excel file\n",
    "    table.to_excel(table_path, index=False)\n",
    "\n",
    "# Remove empty cells at the end as per Rule 3 (clear divisions / remove clutter)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M67lml5Scrmb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1693710093551,
     "user_tz": 300,
     "elapsed": 777,
     "user": {
      "displayName": "Cristian Alejandro Pulido Quintero",
      "userId": "14005360977426477184"
     }
    },
    "outputId": "7b7e1db8-d40b-b9c61751b98b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r100%|██████████| 30/30 [00:00<00:00, 133.75it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------\n",
      "data: 29\n",
      "Termino entrenamiento\n",
      "Termino prediccion y medida EMD\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}